"# 紀錄"
* 實作gradient-desenct
* 盡量不要寫迴圈
* 2016/10/15  gd_test2.py 改寫不要跑迴圈
* 2016/10/25 gd_test_theano.py 用theano寫gd
* 2016/10/26 gd_test_theano_learning_rate.py 比較不同learning rate的影響
* 2016/11/1 mk_batches.py 練習
* 2016/11/2 theano進階寫法練習(all data/mini batch/sgd)

"# 想實作看看的東西"
* adagrad
* XOR
* feature scaling

"# gradient desecnt / stochastic gradient descent"
* 怎麼做gd http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/Regression%20(v6).pdf (對b的偏微分少乘-1，因為ppt動畫關係)
* 怎麼做sgd http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/Gradient%20Descent%20(v2).pdf

"# mini batch"
* 怎麼做mini batch #http://stackoverflow.com/questions/38157972/how-to-implement-mini-batch-gradient-descent-in-python
* mini batch比較快? http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/Keras.pdf
* mini batch比較快? http://speech.ee.ntu.edu.tw/~tlkagk/courses/MLDS_2015_2/Lecture/DNN%20(v4).pdf

"# 不同learning rate的影響"
* http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2016/Lecture/Gradient%20Descent%20(v2).pdf
* ![alt tag](http://cs231n.github.io/assets/nn3/learningrates.jpeg)


